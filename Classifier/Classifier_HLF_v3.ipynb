{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN but with the 50:50 ratio between SM e BSM enforced with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import sys, scipy, pickle\n",
    "from scipy.stats import chi2, poisson\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import ROOT as rt\n",
    "import root_numpy as rtnp\n",
    "from utility_func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN, ModelCheckpoint\n",
    "from keras.constraints import max_norm\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "print keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlf_features = ['HT', 'METp', 'METo', 'MT', 'nJets', \n",
    "                'bJets', 'allJetMass', 'LepPt', 'LepEta', \n",
    "                'LepIsoCh', 'LepIsoGamma', 'LepIsoNeu', 'LepCharge', \n",
    "                'LepIsEle', 'nMu', 'allMuMass', 'allMuPt', 'nEle', \n",
    "                'allEleMass', 'allElePt', 'nChHad', 'nNeuHad', 'nPhoton']\n",
    "\n",
    "active_hlf_features = ['HT', 'allJetMass',\n",
    "                       'allMuPt', 'allMuMass',\n",
    "                       'allElePt','allEleMass',\n",
    "                       'LepPt', 'LepIsoCh', 'LepIsoGamma', 'LepIsoNeu',\n",
    "                       \n",
    "                       'METp', 'METo',\n",
    "                       \n",
    "                       'MT',\n",
    "                       \n",
    "                       'nMu', 'nJets', 'bJets', 'nEle', \n",
    "                       \n",
    "                       'LepCharge','LepIsEle', \n",
    "                       \n",
    "                       'nChHad', 'nNeuHad']\n",
    "\n",
    "Nf_lognorm = 10\n",
    "Nf_gauss = 2\n",
    "Nf_Pgauss = 1\n",
    "Nf_PDgauss = 4\n",
    "Nf_binomial = 2\n",
    "Nf_poisson = 2\n",
    "\n",
    "sel = []\n",
    "for a in active_hlf_features:\n",
    "    for i,f in enumerate(hlf_features):\n",
    "        if a == f:\n",
    "            sel.append(i)\n",
    "# print 'Activated HLF:'\n",
    "# for n in np.array(hlf_features)[sel]:\n",
    "#     print '\\t', n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../data/HLF_ONLY/'\n",
    "TrainSamplesName = ['Wlnu', 'qcd', 'Zll', 'ttbar']\n",
    "BSM_samples = ['Ato4l', 'Zprime', 'Wprime', 'leptoquark', 'hToTauTau', 'hChToTauNu']\n",
    "\n",
    "N_train_max = int(9e6)\n",
    "training_split_fraction = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fraction = np.array([0.592, 0.338, 0.067, 0.003])\n",
    "\n",
    "raw_sample = {}\n",
    "l = np.zeros(4)\n",
    "for i,n in enumerate(TrainSamplesName):\n",
    "    raw_sample[n] = np.load(data_folder+n+'_sample.npy')\n",
    "#     np.random.shuffle(raw_sample[n])\n",
    "    l[i] = raw_sample[n].shape[0]\n",
    "    \n",
    "i_min = np.argmin(l/fraction)\n",
    "if TrainSamplesName[i_min]=='qcd':\n",
    "    print 'QCD is limiting, using it for both val and split'\n",
    "    N_train = min(N_train_max, l[i_min]/fraction[i_min])\n",
    "else:\n",
    "    N_train = min(N_train_max, training_split_fraction*l[i_min]/fraction[i_min])\n",
    "    \n",
    "if N_train < N_train_max:\n",
    "    print 'Limiting stat. sample:', TrainSamplesName[i_min]\n",
    "else:\n",
    "    print 'Sample available satisfying '\n",
    "\n",
    "N_val = N_train*(1-training_split_fraction)/training_split_fraction - 1\n",
    "print 'Expected {:.2f} train'.format(N_train/1.0e6)\n",
    "print 'Expected {:.2f} val'.format(N_val/1.0e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_s = {}\n",
    "x_val_s = {}\n",
    "\n",
    "table = PrettyTable(['Sample', 'Evts tot', 'Train', 'Val'])\n",
    "\n",
    "for i,n in enumerate(TrainSamplesName):\n",
    "    N_train_aux = int(N_train * fraction[i])\n",
    "    x_train_s[n] = raw_sample[n][:N_train_aux, sel]\n",
    "    N_val_aux = int(N_val * fraction[i])\n",
    "    if TrainSamplesName[i_min]=='qcd' and n == 'qcd':\n",
    "        print 'QCD is limiting, using it for both val and split'\n",
    "        np.random.shuffle(raw_sample[n])\n",
    "        x_val_s[n] = raw_sample[n][:N_val_aux, sel]\n",
    "    elif N_train_aux+N_val_aux < raw_sample[n].shape[0]:\n",
    "        x_val_s[n] = raw_sample[n][N_train_aux : N_train_aux+N_val_aux, sel]\n",
    "    else:\n",
    "        print 'Error', n\n",
    "        continue\n",
    "    table.add_row([n, raw_sample[n].shape[0], x_train_s[n].shape[0], x_val_s[n].shape[0]])\n",
    "print table\n",
    "    \n",
    "x_train = np.concatenate((x_train_s['Wlnu'], x_train_s['qcd'], x_train_s['Zll'], x_train_s['ttbar']))\n",
    "x_val = np.concatenate((x_val_s['Wlnu'], x_val_s['qcd'], x_val_s['Zll'], x_val_s['ttbar']))\n",
    "\n",
    "print 'Tot training {:.2f} M'.format(x_train.shape[0]/1.0e6)\n",
    "print 'Tot val {:.2f} M'.format(x_val.shape[0]/1.0e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sig_train = {}\n",
    "x_sig_val = {}\n",
    "weight_sig = {}\n",
    "for n in BSM_samples:\n",
    "    s = np.load(data_folder+n+'_sample.npy')\n",
    "    N_aux = min(int(N_train_max/training_split_fraction), s.shape[0])\n",
    "    N_train_aux = int(N_aux*training_split_fraction)\n",
    "    N_val_aux = N_aux - N_train_aux\n",
    "    x_sig_train[n] = s[:N_train_aux, sel]\n",
    "    x_sig_val[n] = s[N_train_aux:N_train_aux+N_val_aux, sel]\n",
    "    w = float(x_train.shape[0])/x_sig_train[n].shape[0]\n",
    "    weight_sig[n] = w\n",
    "    print '{} {:.3f}M ({:.1f})'.format(n, s.shape[0]*1.e-6, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the Classifier declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = x_train.shape[1]\n",
    "act_fun = 'relu'\n",
    "clf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global w_BSM\n",
    "w_BSM = 1.0\n",
    "\n",
    "def weighted_binary_xentropy(y_true, y_pred):\n",
    "    print w_BSM\n",
    "    aux = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    w = K.tf.where(K.tf.greater(y_true, 0.5), K.tf.ones_like(y_true)*w_BSM, K.tf.ones_like(y_true))\n",
    "    return K.dot(K.transpose(w), aux)/K.sum(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(h, tag='', save_values=False):\n",
    "    f = plt.figure(figsize=(16,8))\n",
    "    plt.plot(h['loss'][1:], '.-', label='loss')\n",
    "    plt.plot(h['val_loss'][1:], '.--', label='val_loss')\n",
    "\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.grid()\n",
    "    plt.legend(loc='best')\n",
    "    plt.yscale('log')\n",
    "    if tag:\n",
    "        f.savefig('Classifier_HLF_v3/{}_training_hisotry.png'.format(tag))\n",
    "        if save_values:\n",
    "            f = open('Classifier_HLF_v3/{}_training_hisotry.pkl'.format(tag), 'w')\n",
    "            pickle.dump(h, f, pickle.HIGHEST_PROTOCOL)\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_it(model, n, ES_p=30, RLRP_p=10, epochs=100):\n",
    "    clf[n] = model\n",
    "\n",
    "    global w_BSM\n",
    "    w_BSM = weight_sig[n]\n",
    "    clf[n].compile(loss=weighted_binary_xentropy, optimizer='adam')\n",
    "#     clf[n].compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    print '\\n\\n------------------ Training', n, '--------------------'\n",
    "    aux_x = np.concatenate(( x_train, x_sig_train[n] ))\n",
    "    aux_y = np.concatenate(( np.zeros(x_train.shape[0]), np.ones(x_sig_train[n].shape[0]) ))\n",
    "    aux_xval = np.concatenate(( x_val, x_sig_val[n] ))\n",
    "    aux_yval = np.concatenate(( np.zeros(x_val.shape[0]), np.ones(x_sig_val[n].shape[0]) ))\n",
    "\n",
    "    clf[n].report = clf[n].fit(x=aux_x, y=aux_y, batch_size=500, epochs=epochs,\n",
    "                          shuffle=True,\n",
    "                          validation_data= (aux_xval, aux_yval),\n",
    "                          callbacks = [\n",
    "                                        EarlyStopping(monitor='val_loss', patience=ES_p, verbose=1),\n",
    "                                        ReduceLROnPlateau(monitor='val_loss', patience=RLRP_p, verbose=1),\n",
    "                                        TerminateOnNaN(),\n",
    "                                        ModelCheckpoint('Classifier_HLF_v3/Clf_{}_best.hdf5'.format(n), \n",
    "                                                    monitor='val_loss',\n",
    "                                                    save_best_only=True, \n",
    "                                                    save_weights_only=True)\n",
    "                                      ]\n",
    "                         )\n",
    "    plot_history(clf[n].report.history, n, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 'Ato4l'\n",
    "\n",
    "if do_training:\n",
    "    inputs = Input(shape=(original_dim,), name='Input_'+n)\n",
    "    mdl = Dense(20, activation=act_fun, name='h1_'+n)(inputs)\n",
    "    #     mdl = Dropout(0.2)(mdl)\n",
    "    mdl = Dense(20, activation=act_fun, name='h2_'+n)(mdl)\n",
    "#     mdl = Dropout(0.1)(mdl)\n",
    "    mdl = Dense(1, activation='sigmoid', name='out_'+n)(mdl)\n",
    "    train_it(Model(inputs=inputs, outputs=mdl), n)\n",
    "else:\n",
    "    clf[n] = load_model('Classifier_HLF_v3/Classifier_'+n+'_v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 'Zprime'\n",
    "if do_training:\n",
    "    inputs = Input(shape=(original_dim,), name='Input_'+n)\n",
    "    mdl = Dense(50, activation=act_fun, name='h1_'+n, kernel_constraint=max_norm(2.))(inputs)\n",
    "#     mdl = Dropout(0.4)(mdl)\n",
    "    mdl = Dense(50, activation=act_fun, name='h2_'+n, kernel_constraint=max_norm(2.))(mdl)\n",
    "#     mdl = Dropout(0.4)(mdl)\n",
    "    mdl = Dense(1, activation='sigmoid', name='out_'+n)(mdl)\n",
    "    train_it(Model(inputs=inputs, outputs=mdl), n)\n",
    "else:\n",
    "    clf[n] = load_model('Classifier_'+n+'_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 'Wprime'\n",
    "if do_training:\n",
    "    inputs = Input(shape=(original_dim,), name='Input_'+n)\n",
    "    mdl = Dense(50, activation=act_fun, name='h1_'+n, kernel_constraint=max_norm(2.))(inputs)\n",
    "#     mdl = Dropout(0.4)(mdl)\n",
    "    mdl = Dense(50, activation=act_fun, name='h2_'+n, kernel_constraint=max_norm(2.))(mdl)\n",
    "#     mdl = Dropout(0.5)(mdl)\n",
    "    mdl = Dense(1, activation='sigmoid', name='out_'+n)(mdl)\n",
    "    train_it(Model(inputs=inputs, outputs=mdl), n)\n",
    "else:\n",
    "    clf[n]= load_model('Classifier_'+n+'_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 'leptoquark'\n",
    "if do_training:\n",
    "    intermediate_dim = 40\n",
    "    inputs = Input(shape=(original_dim,), name='Input_'+n)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h1_'+n, kernel_constraint=max_norm(2.))(inputs)\n",
    "#     mdl = Dropout(0.3)(mdl)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h2_'+n, kernel_constraint=max_norm(2.))(mdl)\n",
    "#     mdl = Dropout(0.4)(mdl)\n",
    "    mdl = Dense(1, activation='sigmoid', name='out_'+n)(mdl)\n",
    "    train_it(Model(inputs=inputs, outputs=mdl), n)\n",
    "else:\n",
    "    clf[n] = load_model('Classifier_'+n+'_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 'hToTauTau'\n",
    "if do_training:\n",
    "    intermediate_dim = 40\n",
    "    inputs = Input(shape=(original_dim,), name='Input_'+n)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h1_'+n, kernel_constraint=max_norm(2.))(inputs)\n",
    "#     mdl = Dropout(0.3)(mdl)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h2_'+n, kernel_constraint=max_norm(2.))(mdl)\n",
    "#     mdl = Dropout(0.4)(mdl)\n",
    "    mdl = Dense(1, activation='sigmoid', name='out_'+n)(mdl)\n",
    "    train_it(Model(inputs=inputs, outputs=mdl), n)\n",
    "else:\n",
    "    clf[n] = load_model('Classifier_'+n+'_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n = 'hChToTauNu'\n",
    "if do_training:\n",
    "    intermediate_dim = 40\n",
    "    inputs = Input(shape=(original_dim,), name='Input_'+n)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h1_'+n, kernel_constraint=max_norm(2.))(inputs)\n",
    "#     mdl = Dropout(0.3)(mdl)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h2_'+n, kernel_constraint=max_norm(2.))(mdl)\n",
    "#     mdl = Dropout(0.4)(mdl)\n",
    "    mdl = Dense(1, activation='sigmoid', name='out_'+n)(mdl)\n",
    "    train_it(Model(inputs=inputs, outputs=mdl), n)\n",
    "else:\n",
    "    clf[n] = load_model('Classifier_'+n+'_v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eff_SM = 5.38e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve(p_BSM, p_SM, eval_q_SM):\n",
    "    eval_p = np.percentile(1-p_SM, q=100*eval_q_SM)\n",
    "    \n",
    "    out = (1-p_BSM) < eval_p\n",
    "    out = np.sum(out, axis=0)\n",
    "    q_BSM = out/float(p_BSM.shape[0])\n",
    "    \n",
    "    AUC = np.trapz(q_BSM, eval_q_SM)\n",
    "    \n",
    "    return q_BSM, AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "f, ax_arr = plt.subplots(3,2 , figsize=(18,18))\n",
    "\n",
    "f_ROC, ax_arr_ROC = plt.subplots(1,1, figsize=(10,10))\n",
    "\n",
    "for i,n in enumerate(BSM_samples):\n",
    "    p_SM = clf[n].predict(x_val, batch_size=2000)\n",
    "    \n",
    "    l_bsm = int(x_sig[n].shape[0]*training_split_fraction)\n",
    "    p_BSM = clf[n].predict(x_sig[n][l_bsm:], batch_size=2000)\n",
    "    \n",
    "    # print ROC curve\n",
    "    q_SM = np.logspace(base=10, start=-7, stop=0, num=100)\n",
    "    q_BSM, roc_auc = ROC_curve(p_BSM, p_SM, q_SM)\n",
    "    dic2save = {'eff_BSM':q_BSM, 'eff_SM':q_SM, 'roc_auc':roc_auc}\n",
    "    fileout = open('Classifier_HLF_v3/ROC_dict_{}.pkl'.format(n), 'w')\n",
    "    pickle.dump(file=fileout, obj=dic2save)\n",
    "    fileout.close()\n",
    "    ax_arr_ROC.plot(q_SM, q_BSM, label='{} (area = {:0.2f})'.format(n, roc_auc))\n",
    "    \n",
    "    \n",
    "\n",
    "    q = np.percentile(p_SM, 100*(1-Eff_SM))\n",
    "    eff_BSM = float(np.sum(p_BSM>q))/x_sig[n][l_bsm:].shape[0]\n",
    "\n",
    "    print '{}: {:1.2e}'.format(n, eff_BSM)\n",
    "    \n",
    "    bins = np.logspace(base=10, start=-5, stop=0, num=50)\n",
    "    if n=='Wprime':\n",
    "        bins = np.logspace(base=10, start=-2, stop=0, num=50)\n",
    "    ax_arr[i/2,i%2].hist(1-p_SM, bins=bins, alpha = 0.5, label='SMMix')\n",
    "    ax_arr[i/2,i%2].hist(1-p_BSM, bins=bins, alpha=0.5, label=n)\n",
    "    ax_arr[i/2,i%2].plot([1-q,1-q], [0,1e6], '--r', label='cut')\n",
    "\n",
    "    ax_arr[i/2,i%2].set_title('{} classifier, eff {:1.2e}'.format(n, eff_BSM))\n",
    "    ax_arr[i/2,i%2].legend(loc='best')\n",
    "    ax_arr[i/2,i%2].set_ylabel('Events')\n",
    "    ax_arr[i/2,i%2].set_xlabel('Probability SM predicted')\n",
    "    ax_arr[i/2,i%2].set_yscale('log')\n",
    "    ax_arr[i/2,i%2].set_xscale('log')\n",
    "    \n",
    "ax_arr_ROC.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax_arr_ROC.plot([Eff_SM, Eff_SM], [1e-6, 1.05], color='red', lw=2, linestyle='--', label='1000 SM evts/month')\n",
    "ax_arr_ROC.set_xlim([1e-6, 1.0])\n",
    "ax_arr_ROC.set_ylim([1e-6, 1.05])\n",
    "ax_arr_ROC.set_xlabel('SM efficiency')\n",
    "ax_arr_ROC.set_ylabel('BSM efficiency')\n",
    "ax_arr_ROC.set_title('ROC supervised training')\n",
    "ax_arr_ROC.legend(loc=\"lower right\")\n",
    "ax_arr_ROC.set_yscale('log')\n",
    "ax_arr_ROC.set_xscale('log')\n",
    "ax_arr_ROC.grid()\n",
    "f_ROC.savefig('Classifier_HLF_v3/ROC_clf.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,c in clf.iteritems():\n",
    "    c.save('Classifier_HLF_v3/Classifier_'+k+'_v3.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
