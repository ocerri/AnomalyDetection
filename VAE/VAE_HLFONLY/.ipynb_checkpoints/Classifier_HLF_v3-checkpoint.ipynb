{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DNN but with the 50:50 ratio between SM e BSM enforced with weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import sys, scipy\n",
    "from scipy.stats import chi2, poisson\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "from prettytable import PrettyTable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.10/08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/olmo/programs/root-6.10.08/lib/ROOT.py:318: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  return _orig_ihook( name, *args, **kwds )\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ROOT as rt\n",
    "import root_numpy as rtnp\n",
    "from utility_func import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.2\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Dropout\n",
    "from keras.models import Model\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "from keras.constraints import max_norm\n",
    "from keras.models import load_model\n",
    "import keras\n",
    "print keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20.0\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print sklearn.__version__\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlf_features = ['HT', 'METp', 'METo', 'MT', 'nJets', \n",
    "                'bJets', 'allJetMass', 'LepPt', 'LepEta', \n",
    "                'LepIsoCh', 'LepIsoGamma', 'LepIsoNeu', 'LepCharge', \n",
    "                'LepIsEle', 'nMu', 'allMuMass', 'allMuPt', 'nEle', \n",
    "                'allEleMass', 'allElePt', 'nChHad', 'nNeuHad', 'nPhoton']\n",
    "\n",
    "active_hlf_features = ['HT', 'allJetMass',\n",
    "                       'allMuPt', 'allMuMass',\n",
    "                       'allElePt','allEleMass',\n",
    "                       'LepPt', 'LepIsoCh', 'LepIsoGamma', 'LepIsoNeu',\n",
    "                       \n",
    "                       'METp', 'METo',\n",
    "                       \n",
    "                       'MT',\n",
    "                       \n",
    "                       'nMu', 'nJets', 'bJets', 'nEle', \n",
    "                       \n",
    "                       'LepCharge','LepIsEle', \n",
    "                       \n",
    "                       'nChHad', 'nNeuHad']\n",
    "\n",
    "Nf_lognorm = 10\n",
    "Nf_gauss = 2\n",
    "Nf_Pgauss = 1\n",
    "Nf_PDgauss = 4\n",
    "Nf_binomial = 2\n",
    "Nf_poisson = 2\n",
    "\n",
    "sel = []\n",
    "for a in active_hlf_features:\n",
    "    for i,f in enumerate(hlf_features):\n",
    "        if a == f:\n",
    "            sel.append(i)\n",
    "# print 'Activated HLF:'\n",
    "# for n in np.array(hlf_features)[sel]:\n",
    "#     print '\\t', n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../../data/HLF_ONLY/'\n",
    "TrainSamplesName = ['Wlnu', 'qcd', 'Zll', 'ttbar']\n",
    "BSM_samples = ['Ato4l', 'Zprime', 'Wprime', 'leptoquark', 'hToTauTau', 'hChToTauNu']\n",
    "\n",
    "N_train_max = int(5e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting stat. sample: Wlnu\n",
      "+--------+----------+---------+--------+\n",
      "| Sample | Evts tot |  Train  |  Val   |\n",
      "+--------+----------+---------+--------+\n",
      "|  Wlnu  | 5003421  | 4002736 | 800546 |\n",
      "|  qcd   | 3200047  | 2285346 | 457068 |\n",
      "|  Zll   | 1777383  |  453012 | 90601  |\n",
      "| ttbar  | 5001726  |  20284  |  4055  |\n",
      "+--------+----------+---------+--------+\n",
      "Tot training 6761378\n",
      "Tot val 1352270\n"
     ]
    }
   ],
   "source": [
    "fraction = np.array([0.592, 0.338, 0.067, 0.003])\n",
    "\n",
    "raw_sample = {}\n",
    "l = np.zeros(4)\n",
    "for i,n in enumerate(TrainSamplesName):\n",
    "    raw_sample[n] = np.load(data_folder+n+'_lepFilter_13TeV_sample.npy')\n",
    "    l[i] = raw_sample[n].shape[0]\n",
    "    \n",
    "i_min = np.argmin(l/fraction)\n",
    "print 'Limiting stat. sample:', TrainSamplesName[i_min]\n",
    "\n",
    "N_train = min(0.8*l[i_min], N_train_max)\n",
    "\n",
    "x_train_s = {}\n",
    "x_val_s = {}\n",
    "\n",
    "table = PrettyTable(['Sample', 'Evts tot', 'Train', 'Val'])\n",
    "\n",
    "for i,n in enumerate(TrainSamplesName):\n",
    "    N_train_aux = int(N_train * fraction[i]/fraction[i_min])\n",
    "    x_train_s[n] = raw_sample[n][:N_train_aux, sel]\n",
    "    x_val_s[n] = raw_sample[n][N_train_aux : int(N_train_aux*1.2) - 1, sel]\n",
    "    table.add_row([n, raw_sample[n].shape[0], N_train_aux, x_val_s[n].shape[0]])\n",
    "print table\n",
    "    \n",
    "x_train = np.concatenate((x_train_s['Wlnu'], x_train_s['qcd'], x_train_s['Zll'], x_train_s['ttbar']))\n",
    "x_val = np.concatenate((x_val_s['Wlnu'], x_val_s['qcd'], x_val_s['Zll'], x_val_s['ttbar']))\n",
    "\n",
    "print 'Tot training {}'.format(x_train.shape[0])\n",
    "print 'Tot val {}'.format(x_val.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ato4l 141.7241972\n",
      "Zprime 122.130305986\n",
      "Wprime 11.1636160102\n",
      "leptoquark 24.7122775983\n",
      "hToTauTau 19.6029131646\n",
      "hChToTauNu 19.9440088727\n"
     ]
    }
   ],
   "source": [
    "x_sig = {}\n",
    "weight_sig = {}\n",
    "for n in BSM_samples:\n",
    "    s = np.load(data_folder+n+'_lepFilter_13TeV_sample.npy')\n",
    "    x_sig[n] = s[:N_train_max, sel]\n",
    "    w = x_train.shape[0]/float(x_sig[n].shape[0])\n",
    "    weight_sig[n] = w\n",
    "    print n, w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the Classifier declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = x_train.shape[1]\n",
    "intermediate_dim = 15\n",
    "act_fun = 'relu'\n",
    "clf = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_BSM = 1.0\n",
    "def weighted_binary_xentropy(y_true, y_pred):\n",
    "    aux = K.binary_crossentropy(y_true, y_pred)\n",
    "\n",
    "    w = y_true*w_BSM + (1 - y_true)    \n",
    "    return K.dot(K.transpose(w), aux)/K.sum(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "------------------ Training Ato4l --------------------\n",
      "Train on 6799544 samples, validate on 1361812 samples\n",
      "Epoch 1/50\n",
      "6799544/6799544 [==============================] - 21s 3us/step - loss: 0.0581 - val_loss: 0.0187\n",
      "Epoch 2/50\n",
      "6799544/6799544 [==============================] - 20s 3us/step - loss: 0.0161 - val_loss: 0.0160\n",
      "Epoch 3/50\n",
      "6799544/6799544 [==============================] - 21s 3us/step - loss: 0.0148 - val_loss: 0.0152\n",
      "Epoch 4/50\n",
      "6799544/6799544 [==============================] - 20s 3us/step - loss: 0.0141 - val_loss: 0.0148\n",
      "Epoch 5/50\n",
      "6799544/6799544 [==============================] - 21s 3us/step - loss: 0.0136 - val_loss: 0.0161\n",
      "Epoch 6/50\n",
      "6799544/6799544 [==============================] - 20s 3us/step - loss: 0.0132 - val_loss: 0.0141\n",
      "Epoch 7/50\n",
      "6799544/6799544 [==============================] - 20s 3us/step - loss: 0.0130 - val_loss: 0.0144\n",
      "Epoch 8/50\n",
      "3656000/6799544 [===============>..............] - ETA: 8s - loss: 0.0130"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-83a9d7503ae4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m                                         \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                                         \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m                                         \u001b[0mTerminateOnNaN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m                                       ]\n\u001b[1;32m     31\u001b[0m                          )\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1200\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1201\u001b[0m                             \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1202\u001b[0;31m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1203\u001b[0m                         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1204\u001b[0m                             \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_slice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_slice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    413\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n = 'Ato4l'\n",
    "\n",
    "if do_training:\n",
    "    inputs = Input(shape=(original_dim,), name='Input_'+n)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h1_'+n)(inputs)\n",
    "    #     mdl = Dropout(0.2)(mdl)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h2_'+n)(mdl)\n",
    "    mdl = Dropout(0.25)(mdl)\n",
    "    mdl = Dense(1, activation='sigmoid', name='out_'+n)(mdl)\n",
    "    clf[n] = Model(inputs=inputs, outputs=mdl)\n",
    "\n",
    "    w_BSM = weight_sig[n]\n",
    "    clf[n].compile(loss=weighted_binary_xentropy, optimizer='adam')\n",
    "#     clf[n].compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    print '\\n\\n------------------ Training', n, '--------------------'\n",
    "    l_bsm = int(x_sig[n].shape[0]*0.8)\n",
    "    aux_x = np.concatenate(( x_train, x_sig[n][:l_bsm] ))\n",
    "    aux_y = np.concatenate(( np.zeros(x_train.shape[0]), np.ones(l_bsm) ))\n",
    "    aux_xval = np.concatenate(( x_val, x_sig[n][l_bsm:] ))\n",
    "    aux_yval = np.concatenate(( np.zeros(x_val.shape[0]), np.ones(x_sig[n].shape[0] - l_bsm) ))\n",
    "\n",
    "    clf[n].training_history = clf[n].fit(x=aux_x, y=aux_y, batch_size=1000, epochs=50,\n",
    "                          shuffle=True,\n",
    "                          validation_data= (aux_xval, aux_yval),\n",
    "                          callbacks = [\n",
    "                                        EarlyStopping(monitor='val_loss', patience=4, verbose=1),\n",
    "                                        ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1),\n",
    "                                        TerminateOnNaN()\n",
    "                                      ]\n",
    "                         )\n",
    "else:\n",
    "    clf[n] = load_model('Classifier_'+n+'_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n = 'Zprime'\n",
    "if do_training:\n",
    "    intermediate_dim = 50\n",
    "    inputs = Input(shape=(original_dim,), name='Input_'+n)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h1_'+n, kernel_constraint=max_norm(2.))(inputs)\n",
    "    mdl = Dropout(0.4)(mdl)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h2_'+n, kernel_constraint=max_norm(2.))(mdl)\n",
    "    mdl = Dropout(0.4)(mdl)\n",
    "    mdl = Dense(1, activation='sigmoid', name='out_'+n)(mdl)\n",
    "    clf[n] = Model(inputs=inputs, outputs=mdl)\n",
    "\n",
    "    clf[n].compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    print '\\n\\n------------------ Training', n, '--------------------'\n",
    "    l_bsm = int(x_sig[n].shape[0]*0.8)\n",
    "    aux_x = np.concatenate(( x_train, x_sig[n][:l_bsm] ))\n",
    "    aux_y = np.concatenate(( np.zeros(x_train.shape[0]), np.ones(l_bsm) ))\n",
    "    aux_xval = np.concatenate(( x_val, x_sig[n][l_bsm:] ))\n",
    "    aux_yval = np.concatenate(( np.zeros(x_val.shape[0]), np.ones(x_sig[n].shape[0] - l_bsm) ))\n",
    "\n",
    "    clf[n].training_history = clf[n].fit(x=aux_x, y=aux_y, batch_size=1000, epochs=40,\n",
    "                          shuffle=True,\n",
    "                          validation_data= (aux_xval, aux_yval),\n",
    "                          callbacks = [\n",
    "                                        EarlyStopping(monitor='val_loss', patience=4, verbose=1),\n",
    "                                        ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1),\n",
    "                                        TerminateOnNaN()\n",
    "                                      ]\n",
    "                         )\n",
    "else:\n",
    "    clf[n] = load_model('Classifier_'+n+'_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 'Wprime'\n",
    "if do_training:\n",
    "    intermediate_dim = 50\n",
    "    inputs = Input(shape=(original_dim,), name='Input_'+n)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h1_'+n, kernel_constraint=max_norm(2.))(inputs)\n",
    "    mdl = Dropout(0.4)(mdl)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h2_'+n, kernel_constraint=max_norm(2.))(mdl)\n",
    "    mdl = Dropout(0.5)(mdl)\n",
    "    mdl = Dense(1, activation='sigmoid', name='out_'+n)(mdl)\n",
    "    clf[n] = Model(inputs=inputs, outputs=mdl)\n",
    "\n",
    "    clf[n].compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    print '\\n\\n------------------ Training', n, '--------------------'\n",
    "    l_bsm = int(x_sig[n].shape[0]*0.8)\n",
    "    aux_x = np.concatenate(( x_train, x_sig[n][:l_bsm] ))\n",
    "    aux_y = np.concatenate(( np.zeros(x_train.shape[0]), np.ones(l_bsm) ))\n",
    "    aux_xval = np.concatenate(( x_val, x_sig[n][l_bsm:] ))\n",
    "    aux_yval = np.concatenate(( np.zeros(x_val.shape[0]), np.ones(x_sig[n].shape[0] - l_bsm) ))\n",
    "\n",
    "    clf[n].training_history = clf[n].fit(x=aux_x, y=aux_y, batch_size=1000, epochs=30,\n",
    "                          shuffle=True,\n",
    "                          validation_data= (aux_xval, aux_yval),\n",
    "                          callbacks = [\n",
    "                                        EarlyStopping(monitor='val_loss', patience=3, verbose=1),\n",
    "                                        ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1),\n",
    "                                        TerminateOnNaN()\n",
    "                                      ]\n",
    "                         )\n",
    "else:\n",
    "    clf[n]= load_model('Classifier_'+n+'_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 'leptoquark'\n",
    "if do_training:\n",
    "    intermediate_dim = 40\n",
    "    inputs = Input(shape=(original_dim,), name='Input_'+n)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h1_'+n, kernel_constraint=max_norm(2.))(inputs)\n",
    "    mdl = Dropout(0.3)(mdl)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h2_'+n, kernel_constraint=max_norm(2.))(mdl)\n",
    "    mdl = Dropout(0.4)(mdl)\n",
    "    mdl = Dense(1, activation='sigmoid', name='out_'+n)(mdl)\n",
    "    clf[n] = Model(inputs=inputs, outputs=mdl)\n",
    "\n",
    "    clf[n].compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    print '\\n\\n------------------ Training', n, '--------------------'\n",
    "    l_bsm = int(x_sig[n].shape[0]*0.8)\n",
    "    aux_x = np.concatenate(( x_train, x_sig[n][:l_bsm] ))\n",
    "    aux_y = np.concatenate(( np.zeros(x_train.shape[0]), np.ones(l_bsm) ))\n",
    "    aux_xval = np.concatenate(( x_val, x_sig[n][l_bsm:] ))\n",
    "    aux_yval = np.concatenate(( np.zeros(x_val.shape[0]), np.ones(x_sig[n].shape[0] - l_bsm) ))\n",
    "\n",
    "    clf[n].training_history = clf[n].fit(x=aux_x, y=aux_y, batch_size=500, epochs=40,\n",
    "                          shuffle=True,\n",
    "                          validation_data= (aux_xval, aux_yval),\n",
    "                          callbacks = [\n",
    "                                        EarlyStopping(monitor='val_loss', patience=4, verbose=1),\n",
    "                                        ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1),\n",
    "                                        TerminateOnNaN()\n",
    "                                      ]\n",
    "                         )\n",
    "else:\n",
    "    clf[n] = load_model('Classifier_'+n+'_v1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "do_training = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aux_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sig[n].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 'hToTauTau'\n",
    "if do_training:\n",
    "    intermediate_dim = 40\n",
    "    inputs = Input(shape=(original_dim,), name='Input_'+n)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h1_'+n, kernel_constraint=max_norm(2.))(inputs)\n",
    "    mdl = Dropout(0.3)(mdl)\n",
    "    mdl = Dense(intermediate_dim, activation=act_fun, name='h2_'+n, kernel_constraint=max_norm(2.))(mdl)\n",
    "    mdl = Dropout(0.4)(mdl)\n",
    "    mdl = Dense(1, activation='sigmoid', name='out_'+n)(mdl)\n",
    "    clf[n] = Model(inputs=inputs, outputs=mdl)\n",
    "\n",
    "    clf[n].compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    print '\\n\\n------------------ Training', n, '--------------------'\n",
    "    l_bsm = int(x_sig[n].shape[0]*0.8)\n",
    "    aux_x = np.concatenate(( x_train, x_sig[n][:l_bsm] ))\n",
    "    aux_y = np.concatenate(( np.zeros(x_train.shape[0]), np.ones(l_bsm) ))\n",
    "    aux_xval = np.concatenate(( x_val, x_sig[n][l_bsm:] ))\n",
    "    aux_yval = np.concatenate(( np.zeros(x_val.shape[0]), np.ones(x_sig[n].shape[0] - l_bsm) ))\n",
    "\n",
    "    clf[n].training_history = clf[n].fit(x=aux_x, y=aux_y, batch_size=500, epochs=40,\n",
    "                          shuffle=True,\n",
    "                          validation_data= (aux_xval, aux_yval),\n",
    "                          callbacks = [\n",
    "                                        EarlyStopping(monitor='val_loss', patience=4, verbose=1),\n",
    "                                        ReduceLROnPlateau(monitor='val_loss', patience=2, verbose=1),\n",
    "                                        TerminateOnNaN()\n",
    "                                      ]\n",
    "                         )\n",
    "else:\n",
    "    clf[n] = load_model('Classifier_'+n+'_v1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eff_SM = 5.38e-06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 'Wprime'\n",
    "p_SM = clf[n].predict(x_train, batch_size=2000)\n",
    "p_BSM = clf[n].predict(x_sig[n], batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(1-p_SM, bins=50)\n",
    "plt.yscale('log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ROC_curve(p_BSM, p_SM, eval_q_SM):\n",
    "    eval_p = np.percentile(1-p_SM, q=100*eval_q_SM)\n",
    "    \n",
    "    out = (1-p_BSM) < eval_p\n",
    "    out = np.sum(out, axis=0)\n",
    "    q_BSM = out/float(p_BSM.shape[0])\n",
    "    \n",
    "    AUC = np.trapz(q_BSM, eval_q_SM)\n",
    "    \n",
    "    return q_BSM, AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "matplotlib.rcParams.update({'font.size': 16})\n",
    "\n",
    "f, ax_arr = plt.subplots(2,2 , figsize=(18,18))\n",
    "\n",
    "f_ROC, ax_arr_ROC = plt.subplots(1,1, figsize=(10,10))\n",
    "\n",
    "for i,n in enumerate(BSM_samples):\n",
    "    p_SM = clf[n].predict(x_train, batch_size=2000)\n",
    "    p_BSM = clf[n].predict(x_sig[n], batch_size=2000)\n",
    "    \n",
    "    # print ROC curve\n",
    "    q_SM = np.logspace(base=10, start=-7, stop=0, num=100)\n",
    "    q_BSM, roc_auc = ROC_curve(p_BSM, p_SM, q_SM)\n",
    "    \n",
    "    \n",
    "#     ax_arr[i,0].plot(q_SM, q_BSM, color='darkorange', label='ROC curve (area = {:0.2f})'.format(roc_auc))\n",
    "#     ax_arr[i,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "#     ax_arr[i,0].plot([Eff_SM, Eff_SM], [1e-6, 1.05], color='red', lw=2, linestyle='--')\n",
    "#     ax_arr[i,0].set_xlim([1e-6, 1.0])\n",
    "#     ax_arr[i,0].set_ylim([1e-6, 1.05])\n",
    "#     ax_arr[i,0].set_xlabel('SM efficiency')\n",
    "#     ax_arr[i,0].set_ylabel(n+' efficiency')\n",
    "#     ax_arr[i,0].set_title('ROC - ' + n)\n",
    "#     ax_arr[i,0].legend(loc=\"lower right\")\n",
    "#     ax_arr[i,0].set_yscale('log')\n",
    "#     ax_arr[i,0].set_xscale('log')\n",
    "#     ax_arr[i,0].grid()\n",
    "    ax_arr_ROC.plot(q_SM, q_BSM, label='{} (area = {:0.2f})'.format(n, roc_auc))\n",
    "    \n",
    "    \n",
    "\n",
    "    q = np.percentile(p_SM, 100*(1-Eff_SM))\n",
    "    eff_BSM = float(np.sum(p_BSM>q))/x_sig[n].shape[0]\n",
    "\n",
    "    print '{}: {:1.2e}'.format(n, eff_BSM)\n",
    "    \n",
    "    bins = np.logspace(base=10, start=-4, stop=0, num=50)\n",
    "    if n=='Wprime':\n",
    "        bins = np.logspace(base=10, start=-2, stop=0, num=50)\n",
    "    ax_arr[i/2,i%2].hist(1-p_SM, bins=bins, alpha = 0.5, label='SMMix')\n",
    "    ax_arr[i/2,i%2].hist(1-p_BSM, bins=bins, alpha=0.5, label=n)\n",
    "    ax_arr[i/2,i%2].plot([1-q,1-q], [0,1e6], '--r', label='cut')\n",
    "\n",
    "    ax_arr[i/2,i%2].set_title('{} classifier, eff {:1.2e}'.format(n, eff_BSM))\n",
    "    ax_arr[i/2,i%2].legend(loc='best')\n",
    "    ax_arr[i/2,i%2].set_ylabel('Events')\n",
    "    ax_arr[i/2,i%2].set_xlabel('Probability SM predicted')\n",
    "    ax_arr[i/2,i%2].set_yscale('log')\n",
    "    ax_arr[i/2,i%2].set_xscale('log')\n",
    "    \n",
    "ax_arr_ROC.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "ax_arr_ROC.plot([Eff_SM, Eff_SM], [1e-6, 1.05], color='red', lw=2, linestyle='--', label='1000 SM evts/month')\n",
    "ax_arr_ROC.set_xlim([1e-6, 1.0])\n",
    "ax_arr_ROC.set_ylim([1e-6, 1.05])\n",
    "ax_arr_ROC.set_xlabel('SM efficiency')\n",
    "ax_arr_ROC.set_ylabel('BSM efficiency')\n",
    "ax_arr_ROC.set_title('ROC supervised training')\n",
    "ax_arr_ROC.legend(loc=\"lower right\")\n",
    "ax_arr_ROC.set_yscale('log')\n",
    "ax_arr_ROC.set_xscale('log')\n",
    "ax_arr_ROC.grid()\n",
    "f_ROC.savefig('plots/ROC_clf_v1.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,c in clf.iteritems():\n",
    "    c.save('Classifier_'+k+'_v1.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
