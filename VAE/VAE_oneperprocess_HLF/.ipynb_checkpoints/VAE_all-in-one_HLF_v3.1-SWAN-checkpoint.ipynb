{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "VampPrior\n",
    "\n",
    "- Tried with 10 hidden dim -> not converged\n",
    "- Tried to add allLepMass and hidden from 4 to 5 -> Eff improve but reco is a shit\n",
    "- If remove VampEff decrease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import sys, scipy\n",
    "from scipy.stats import chi2, poisson\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.12/06\n"
     ]
    }
   ],
   "source": [
    "import ROOT as rt\n",
    "import root_numpy as rtnp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.2\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import layers as KL\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN\n",
    "import keras\n",
    "print keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "hlf_features = ['HT', 'MET', 'PhiMET', 'MT', 'nJets', 'bJets',\n",
    "                'allJetMass', 'LepPt', 'LepEta', 'LepPhi', 'LepIsoCh',\n",
    "                'LepIsoGamma', 'LepIsoNeu', 'LepCharge', 'LepIsEle', 'nMu',\n",
    "                'allMuMass', 'allMuPt', 'nEle', 'allEleMass', 'allElePt', 'nChHad',\n",
    "                'nNeuHad', 'nPhoton']\n",
    "\n",
    "active_hlf_features = ['HT', 'MET', 'allJetMass',\n",
    "                       'allMuPt','allElePt','allEleMass', 'allMuMass',\n",
    "                       'LepPt', 'LepIsoCh', 'LepIsoGamma', 'LepIsoNeu',\n",
    "                       \n",
    "                       'LepEta',\n",
    "                       \n",
    "                       'MT',\n",
    "                       \n",
    "                       'nMu', 'nJets', 'bJets', 'nEle', \n",
    "                       \n",
    "                       'LepCharge','LepIsEle', \n",
    "                       \n",
    "                       'nChHad', 'nNeuHad', 'nPhoton']\n",
    "\n",
    "Nf_lognorm = 11\n",
    "Nf_gauss = 1\n",
    "Nf_Pgauss = 1\n",
    "Nf_PDgauss = 4\n",
    "Nf_binomial = 2\n",
    "Nf_poisson = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Activated HLF:\n",
      "\tHT\n",
      "\tMET\n",
      "\tallJetMass\n",
      "\tallMuPt\n",
      "\tallElePt\n",
      "\tallEleMass\n",
      "\tallMuMass\n",
      "\tLepPt\n",
      "\tLepIsoCh\n",
      "\tLepIsoGamma\n",
      "\tLepIsoNeu\n",
      "\tLepEta\n",
      "\tMT\n",
      "\tnMu\n",
      "\tnJets\n",
      "\tbJets\n",
      "\tnEle\n",
      "\tLepCharge\n",
      "\tLepIsEle\n",
      "\tnChHad\n",
      "\tnNeuHad\n",
      "\tnPhoton\n"
     ]
    }
   ],
   "source": [
    "sel = []\n",
    "for a in active_hlf_features:\n",
    "    for i,f in enumerate(hlf_features):\n",
    "        if a == f:\n",
    "            sel.append(i)\n",
    "\n",
    "print 'Activated HLF:'\n",
    "for n in np.array(hlf_features)[sel]:\n",
    "    print '\\t', n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '../../data/'\n",
    "TrainSamplesName = ['Wlnu', 'qcd', 'ttbar']\n",
    "BSM_samples = ['AtoChHW', 'AtoChHW_HIGHMASS', 'Ato4l', 'Zprime', 'Wprime']\n",
    "\n",
    "N_train_max = int(2e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Limiting stat. sample: qcd\n",
      "Wlnu 1635460 327091\n",
      "qcd 932956 186590\n",
      "ttbar 9329 1864\n"
     ]
    }
   ],
   "source": [
    "trigger_rate = np.array([440., 251., 2.51])\n",
    "fraction = trigger_rate/ np.sum(trigger_rate)\n",
    "\n",
    "raw_sample = {}\n",
    "l = np.zeros(3)\n",
    "for i,n in enumerate(TrainSamplesName):\n",
    "    raw_sample[n] = np.load(data_folder+n+'_lepFilter_13TeV_sample.npy')\n",
    "    l[i] = raw_sample[n].shape[0]\n",
    "    \n",
    "i_min = np.argmin(l/fraction)\n",
    "print 'Limiting stat. sample:', TrainSamplesName[i_min]\n",
    "\n",
    "N_train = min(0.8*l[i_min], N_train_max)\n",
    "\n",
    "x_train_s = {}\n",
    "x_val_s = {}\n",
    "for i,n in enumerate(TrainSamplesName):\n",
    "    N_train_aux = int(N_train * fraction[i]/fraction[i_min])\n",
    "    x_train_s[n] = raw_sample[n][:N_train_aux, sel]\n",
    "    x_val_s[n] = raw_sample[n][N_train_aux : int(N_train_aux*1.2) - 1, sel]\n",
    "    print n, N_train_aux, x_val_s[n].shape[0]\n",
    "    \n",
    "x_train_s = np.concatenate((x_train_s['Wlnu'], x_train_s['qcd'], x_train_s['ttbar']))\n",
    "x_val_s = np.concatenate((x_val_s['Wlnu'], x_val_s['qcd'], x_val_s['ttbar']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the VAE declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dim = x_train.shape[1]\n",
    "latent_dim = 4\n",
    "intermediate_dim = 50\n",
    "act_fun = 'relu'\n",
    "clip_x_to0 = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InverseSquareRootLinearUnit(args, min_value = 5e-3):\n",
    "    return 1. + min_value + K.tf.where(K.tf.greater(args, 0), args, K.tf.divide(args, K.sqrt(1+K.square(args))))\n",
    "\n",
    "def ClippedTanh(x):\n",
    "    return 0.5*(1+0.999*K.tanh(x))\n",
    "\n",
    "def SmashTo0(x):\n",
    "    return 0*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_DNN_input = Input(shape=(original_dim,))\n",
    "hidden_1 = Dense(intermediate_dim, activation=act_fun)(x_DNN_input)\n",
    "hidden_2 = Dense(intermediate_dim, activation=act_fun)(hidden_1)\n",
    "# hidden_3 = Dense(intermediate_dim, activation=act_fun)(hidden_2)\n",
    "\n",
    "L_z_mean = Dense(latent_dim)(hidden_2)\n",
    "\n",
    "L_z_sigma_preActivation = Dense(latent_dim)(hidden_2)\n",
    "L_z_sigma = Lambda(InverseSquareRootLinearUnit)(L_z_sigma_preActivation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_sigma = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                              stddev=1.)\n",
    "    return z_mean + z_sigma * epsilon\n",
    "\n",
    "L_z_latent_DNN = Lambda(sampling)([L_z_mean, L_z_sigma])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_h1 = Dense(intermediate_dim, activation=act_fun)(L_z_latent_DNN)\n",
    "decoder_h2 = Dense(intermediate_dim, activation=act_fun)(decoder_h1)\n",
    "# decoder_h3 = Dense(intermediate_dim, activation=act_fun)(decoder_h2)\n",
    "L_par1 = Dense(original_dim)(decoder_h2)\n",
    "\n",
    "L_par2_preActivation = Dense(Nf_lognorm + Nf_gauss + Nf_Pgauss + Nf_PDgauss)(decoder_h2)\n",
    "L_par2 = Lambda(InverseSquareRootLinearUnit)(L_par2_preActivation)\n",
    "\n",
    "L_par3_preActivation = Dense(Nf_lognorm)(decoder_h2)\n",
    "L_par3 = Lambda(ClippedTanh)(L_par3_preActivation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Prior Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_input = Lambda(SmashTo0)(x_DNN_input)\n",
    "h1_prior = Dense(20, kernel_initializer='zeros', bias_initializer='ones', trainable=False)(fixed_input)\n",
    "# h2_prior = Dense(100, activation=act_fun)(h1_prior)\n",
    "\n",
    "L_prior_mean = Dense(latent_dim, kernel_initializer='zeros', bias_initializer='zeros', trainable=True)(h1_prior)\n",
    "\n",
    "L_prior_sigma_preActivation = Dense(latent_dim, kernel_initializer='zeros', bias_initializer='ones', trainable=True)(h1_prior)\n",
    "L_prior_sigma = Lambda(InverseSquareRootLinearUnit)(L_prior_sigma_preActivation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KL_loss_forVAE(mu, sigma, mu_prior, sigma_prior):\n",
    "    kl_loss = K.tf.multiply(K.square(sigma), K.square(sigma_prior))\n",
    "    kl_loss += K.square(K.tf.divide(mu_prior - mu, sigma_prior))\n",
    "    kl_loss += K.log(K.tf.divide(sigma_prior, sigma)) -1\n",
    "    return K.mean(0.5 * K.sum(kl_loss, axis=-1))\n",
    "\n",
    "def RecoProb_forVAE(x, par1, par2, par3):\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "    \n",
    "    #Log-Normal distributed variables\n",
    "    mu = par1[:,:Nf_lognorm]\n",
    "    sigma = par2[:,:Nf_lognorm]\n",
    "    fraction = par3[:,:Nf_lognorm]\n",
    "    x_clipped = K.clip(x[:,:Nf_lognorm], clip_x_to0, 1e8)\n",
    "    single_NLL = K.tf.where(K.less(x[:,:Nf_lognorm], clip_x_to0), \n",
    "                            -K.log(fraction),\n",
    "                                -K.log(1-fraction)\n",
    "                                + K.log(sigma) \n",
    "                                + K.log(x_clipped)\n",
    "                                + 0.5*K.square(K.tf.divide(K.log(x_clipped) - mu, sigma))\n",
    "                           )\n",
    "    nll_loss += K.sum(single_NLL, axis=-1)\n",
    "    N += Nf_lognorm\n",
    "    \n",
    "    # Gaussian distributed variables\n",
    "    mu = par1[:,N:N+Nf_gauss]\n",
    "    sigma = par2[:,N:N+Nf_gauss]\n",
    "    norm_x = K.tf.divide(x[:,N:N+Nf_gauss] - mu, sigma)\n",
    "    single_NLL = K.log(sigma) + 0.5*K.square(norm_x)\n",
    "    nll_loss += K.sum(single_NLL, axis=-1)\n",
    "    N += Nf_gauss\n",
    "    \n",
    "    # Positive Gaussian distributed variables\n",
    "    mu = par1[:,N:N+Nf_Pgauss]\n",
    "    sigma = par2[:,N:N+Nf_Pgauss]\n",
    "    norm_x = K.tf.divide(x[:,N:N+Nf_Pgauss] - mu, sigma)\n",
    "\n",
    "    sqrt2 = 1.4142135624\n",
    "    aNorm = 1 + 0.5*(1 + K.tf.erf(K.tf.divide(- mu, sigma)/sqrt2))\n",
    "    \n",
    "    single_NLL = K.log(sigma) + 0.5*K.square(norm_x) - K.log(aNorm)\n",
    "    nll_loss += K.sum(single_NLL, axis=-1)\n",
    "    N += Nf_Pgauss\n",
    "    \n",
    "    # Positive Discrete Gaussian distributed variables\n",
    "    mu = par1[:,N:N+Nf_PDgauss]\n",
    "    sigma = par2[:,N:N+Nf_PDgauss]\n",
    "    norm_xp = K.tf.divide(x[:,N:N+Nf_PDgauss] + 0.5 - mu, sigma)\n",
    "    norm_xm = K.tf.divide(x[:,N:N+Nf_PDgauss] - 0.5 - mu, sigma)\n",
    "    sqrt2 = 1.4142135624\n",
    "    single_LL = 0.5*(K.tf.erf(norm_xp/sqrt2) - K.tf.erf(norm_xm/sqrt2))\n",
    "    \n",
    "    norm_0 = K.tf.divide(-0.5 - mu, sigma)\n",
    "    aNorm = 1 + 0.5*(1 + K.tf.erf(norm_0/sqrt2))\n",
    "    \n",
    "    single_NLL = -K.log(K.clip(single_LL, 1e-10, 1e40)) -K.log(aNorm)\n",
    "    nll_loss += K.sum(single_NLL, axis=-1)\n",
    "    N += Nf_PDgauss\n",
    "    \n",
    "    #Binomial distributed variables\n",
    "    p = 0.5*(1+0.98*K.tanh(par1[:, N: N+Nf_binomial]))\n",
    "    single_NLL = -K.tf.where(K.equal(x[:, N: N+Nf_binomial],1), K.log(p), K.log(1-p))\n",
    "    nll_loss += K.sum(single_NLL, axis=-1)\n",
    "    N += Nf_binomial\n",
    "    \n",
    "    #Poisson distributed variables\n",
    "    aux = par1[:, N:]\n",
    "    mu = 1 + K.tf.where(K.tf.greater(aux, 0), aux, K.tf.divide(aux, K.sqrt(1+K.square(aux))))\n",
    "    single_NLL = K.tf.lgamma(x[:, N:]+1) - x[:, N:]*K.log(mu) + mu\n",
    "    nll_loss += K.sum(single_NLL, axis=-1)\n",
    "    \n",
    "    return K.mean(nll_loss)\n",
    "\n",
    "def LossVAE(y_train, NETout):\n",
    "    mu = NETout[:, :latent_dim]\n",
    "    N = latent_dim\n",
    "    \n",
    "    sigma = NETout[:, N: N+latent_dim]\n",
    "    N += latent_dim\n",
    "    \n",
    "    mu_prior = NETout[:, N: N+latent_dim]\n",
    "    N += latent_dim\n",
    "    \n",
    "    sigma_prior = NETout[:, N: N+latent_dim]\n",
    "    N += latent_dim\n",
    "    \n",
    "    par1 = NETout[:, N: N+original_dim]\n",
    "    N += original_dim\n",
    "    \n",
    "    par2 = NETout[:, N: N+Nf_lognorm+Nf_gauss+Nf_Pgauss+Nf_PDgauss]\n",
    "    N += Nf_lognorm+Nf_gauss+Nf_Pgauss+Nf_PDgauss\n",
    "    \n",
    "    par3 = NETout[:, N:N+Nf_lognorm]\n",
    "    \n",
    "    return KL_loss_forVAE(mu, sigma, mu_prior, sigma_prior) + RecoProb_forVAE(y_train[:, :original_dim], par1, par2, par3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RecoProb_metric(y_train, NETout):\n",
    "    N = 4*latent_dim\n",
    "    \n",
    "    par1 = NETout[:, N: N+original_dim]\n",
    "    N += original_dim\n",
    "    \n",
    "    par2 = NETout[:, N: N+Nf_lognorm+Nf_gauss+Nf_Pgauss+Nf_PDgauss]\n",
    "    N += Nf_lognorm+Nf_gauss+Nf_Pgauss+Nf_PDgauss\n",
    "    \n",
    "    par3 = NETout[:, N:N+Nf_lognorm]\n",
    "    \n",
    "    return RecoProb_forVAE(y_train[:, :original_dim], par1, par2, par3)\n",
    "\n",
    "def KL_loss_metric(y_train, NETout):\n",
    "    mu = NETout[:, :latent_dim]\n",
    "    sigma = NETout[:, latent_dim: 2*latent_dim]\n",
    "    mu_prior = NETout[:, 2*latent_dim: 3*latent_dim]\n",
    "    sigma_prior = NETout[:, 3*latent_dim: 4*latent_dim]\n",
    "    return KL_loss_forVAE(mu, sigma, mu_prior, sigma_prior)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "global_outputs = KL.concatenate([L_z_mean, L_z_sigma, L_prior_mean, L_prior_sigma, L_par1, L_par2, L_par3], axis=1)\n",
    "\n",
    "vae = Model(inputs=x_DNN_input, outputs=global_outputs)\n",
    "# rms_prop = optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)\n",
    "vae.compile(optimizer='adam', loss=LossVAE, metrics=[RecoProb_metric, KL_loss_metric])\n",
    "print vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.concatenate((x_train, np.zeros((x_train.shape[0], 66 - x_train.shape[1]))), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fit_report = vae.fit(x=x_train, y=y_train,\n",
    "        validation_split = 0.2,\n",
    "        shuffle=True,\n",
    "        epochs=100,\n",
    "        batch_size=400,\n",
    "        callbacks = [\n",
    "                        EarlyStopping(monitor='val_loss', patience=8, verbose=1, min_delta=0.05),\n",
    "                        ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=4, epsilon=0.1, verbose=1),\n",
    "                        TerminateOnNaN()\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "f = plt.figure(figsize=(16,8))\n",
    "style = {'loss':'--', 'RecoProb_metric': 'o', 'KL_loss_metric': '--'}\n",
    "for item in ['loss', 'RecoProb_metric', 'KL_loss_metric']:\n",
    "    plt.plot(np.array(fit_report.history[item][:])[1:], style[item], label=item)\n",
    "    plt.plot(np.array(fit_report.history['val_'+item][:])[1:], style[item], label='val_'+item)\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    vae.save(SampleName + '_v3-1_SWAN_VAE.h5')\n",
    "else:\n",
    "    from keras.models import load_model\n",
    "    custom_objects = {\n",
    "        'original_dim': x_train.shape[1],\n",
    "        'latent_dim': latent_dim,\n",
    "        'intermediate_dim': intermediate_dim,\n",
    "        'act_fun': act_fun,\n",
    "        'Nf_lognorm' : Nf_lognorm,\n",
    "        'Nf_gauss' : Nf_gauss,\n",
    "        'Nf_Pgauss' : Nf_Pgauss,\n",
    "        'Nf_PDgauss' : Nf_PDgauss,\n",
    "        'Nf_binomial' : Nf_binomial,\n",
    "        'Nf_poisson' : Nf_poisson,\n",
    "        'LossVAE': LossVAE,\n",
    "        'RecoProb_metric': RecoProb_metric,\n",
    "        'KL_loss_metric': KL_loss_metric    \n",
    "    }\n",
    "    \n",
    "    vae = load_model(SampleName + '_v3-1_SWAN_VAE.h5', custom_objects=custom_objects)\n",
    "    print 'Loaded'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get prior means and variance array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    Prior_Dumper = Model(inputs=x_DNN_input, outputs=[L_prior_mean, L_prior_sigma])\n",
    "    Prior_Dumper.save(SampleName + '_v3-1_SWAN_prior.h5')\n",
    "else:\n",
    "    Prior_Dumper = load_model(SampleName + '_v3-1_SWAN_prior.h5', custom_objects=custom_objects)\n",
    "    \n",
    "aux = Prior_Dumper.predict(np.zeros((1, original_dim)))\n",
    "mu_prior = aux[0][0]\n",
    "sigma_prior = aux[1][0]\n",
    "\n",
    "print mu_prior\n",
    "print sigma_prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    encoder = Model(inputs=x_DNN_input, outputs=[L_z_mean, L_z_sigma])\n",
    "    encoder.save(SampleName + '_v3-1_SWAN_encoder.h5')\n",
    "else:\n",
    "    encoder = load_model(SampleName + '_v3-1_SWAN_encoder.h5', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gather the signal data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_sig = {}\n",
    "for n in BSM_samples:\n",
    "    s = np.load(data_folder+n+'_lepFilter_13TeV_sample.npy')\n",
    "    x_sig[n] = s[:N_train, sel]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_encoded = np.array(encoder.predict(x_train))\n",
    "\n",
    "x_sig_encoded = {}\n",
    "for k,v in x_sig.iteritems():\n",
    "    print k\n",
    "    x_sig_encoded[k] = np.array(encoder.predict(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Latent space mean distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_gaussians(x, mu_vec, sigma_vec):\n",
    "    x = np.atleast_2d(x)\n",
    "    if x.shape[0] <= x.shape[1]:\n",
    "        x = x.T\n",
    "    x_norm = (x - mu_vec)/sigma_vec\n",
    "    single_gaus_val = np.exp(-0.5*np.square(x_norm))/(sigma_vec*np.sqrt(2*np.pi))\n",
    "    return np.sum(single_gaus_val, axis=1)/mu_vec.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histos = []\n",
    "canvases = []\n",
    "binning = [70, -8, 8]\n",
    "\n",
    "for kk in range(latent_dim):\n",
    "    c = rt.TCanvas('c'+str(kk), 'c'+str(kk), 600, 400)\n",
    "\n",
    "    lower_bound = mu_prior[kk] - 6*sigma_prior[kk]\n",
    "    upper_bound = mu_prior[kk] + 6*sigma_prior[kk]\n",
    "    h = rt.TH1F('hSM{}_{}'.format(SampleName, kk), SampleName, binning[0], lower_bound, upper_bound)\n",
    "    z = x_train_encoded[0, :, kk]\n",
    "    rtnp.fill_hist(h, z, weights=1/float(z.shape[0])*np.ones_like(z))\n",
    "    h.SetStats(0)\n",
    "    h.SetLineColor(rt.kGreen+4-2)\n",
    "    h.SetFillColorAlpha(rt.kGreen+4-2, 0.7)\n",
    "    h.SetFillStyle(3001)\n",
    "    \n",
    "    h.SetTitle('')\n",
    "    h.SetXTitle('Mean of z_{'+str(kk)+'}')\n",
    "    h.SetYTitle('Probability')\n",
    "    \n",
    "    h.Draw('Bar SAME')\n",
    "    histos.append(h)\n",
    "\n",
    "\n",
    "    colors = [1,2,4,rt.kYellow+2,6,7,8]\n",
    "    # fill\n",
    "    for i, n in enumerate(x_sig.keys()):\n",
    "        h = rt.TH1F('hBSM{}_{}'.format(i, kk), n, binning[0], lower_bound, upper_bound)\n",
    "        z = x_sig_encoded[n][0, :, kk]\n",
    "        rtnp.fill_hist(h, z, weights=1/float(z.shape[0])*np.ones_like(z))\n",
    "\n",
    "        h.SetStats(0)\n",
    "        h.SetLineColor(colors[i])\n",
    "        h.SetLineWidth(2)\n",
    "        h.Draw('SAME')\n",
    "        histos.append(h)\n",
    "\n",
    "    c.BuildLegend()\n",
    "    c.SetGrid()\n",
    "    c.SetLogy()\n",
    "    c.Draw()\n",
    "    canvases.append(c)\n",
    "    \n",
    "fout = rt.TFile('plots/'+SampleName + '_v3-1_SWAN_meanZ.root', 'RECREATE')\n",
    "for obj in canvases + histos:\n",
    "    obj.Write()\n",
    "\n",
    "fout.Close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recostruction sum of pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    autoencoder = Model(inputs=x_DNN_input, outputs=[L_par1, L_par2, L_par3])\n",
    "    autoencoder.save(SampleName + '_v3-1_SWAN_autoencoder.h5')\n",
    "else:\n",
    "    autoencoder = load_model(SampleName + '_v3-1_SWAN_autoencoder.h5', custom_objects=custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pars_ae_train = autoencoder.predict(x_train)\n",
    "# print pars_ae_train.shape\n",
    "\n",
    "pars_ae_sig = {}\n",
    "for k,v in x_sig.iteritems():\n",
    "    print k\n",
    "    pars_ae_sig[k] = autoencoder.predict(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_possion(x_in, mu_vec):\n",
    "    out = np.zeros_like(x_in)\n",
    "    for i, aux in enumerate(x_in):\n",
    "        out[i] = np.sum(poisson.pmf(aux, mu_vec))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum_of_lognorm(x, f, mu_vec, sigma_vec):\n",
    "    x = np.atleast_2d(x)\n",
    "    if x.shape[0] <= x.shape[1]:\n",
    "        x = x.T\n",
    "    \n",
    "    x_clipped = np.clip(x, clip_x_to0, 1e8)\n",
    "    x_norm = (np.log(x_clipped) - mu_vec)/sigma_vec\n",
    "    single_prob = np.where(np.less(x, clip_x_to0),\n",
    "                               f,\n",
    "                               (1-f)*np.exp(-0.5*np.square(x_norm))/(x_clipped*sigma_vec*np.sqrt(2*np.pi))\n",
    "    )\n",
    "    return np.sum(single_prob, axis=1)/mu_vec.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import erf\n",
    "def sum_of_PDgauss(x, mu, sigma):\n",
    "    x = np.atleast_2d(x)\n",
    "    if x.shape[0] <= x.shape[1]:\n",
    "        x = x.T\n",
    "    \n",
    "    zp = (x + 0.5 - mu)/sigma\n",
    "    zm = (x - 0.5 - mu)/sigma\n",
    "    \n",
    "    norm_0 = (-0.5 - mu)/sigma\n",
    "    \n",
    "    aNorm = 1 + 0.5*(1 + erf(norm_0/np.sqrt(2)))\n",
    "    single_prob = aNorm*0.5*(erf(zp/np.sqrt(2)) - erf(zm/np.sqrt(2)))\n",
    "    return np.sum(single_prob, axis=1)/mu.shape[0]\n",
    "\n",
    "def sum_of_Pgauss(x, mu, sigma):\n",
    "    x = np.atleast_2d(x)\n",
    "    if x.shape[0] <= x.shape[1]:\n",
    "        x = x.T\n",
    "    x_norm = (x - mu_vec)/sigma_vec\n",
    "    norm_0 = - mu/sigma\n",
    "    aNorm = 1 + 0.5*(1 + erf(norm_0/np.sqrt(2)))\n",
    "    \n",
    "    single_prob = aNorm*np.exp(-0.5*np.square(x_norm))/(sigma_vec*np.sqrt(2*np.pi))\n",
    "    return np.sum(single_prob, axis=1)/mu.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, ax_arr = plt.subplots(1+int(len(active_hlf_features)/3.),3 , figsize=(18,36))\n",
    "\n",
    "for i,hlf_name in enumerate(active_hlf_features):\n",
    "    print hlf_name\n",
    "#     plt.yscale('log', nonposy='clip')\n",
    "    \n",
    "    #Plot distribution of the variable\n",
    "    x_aux = x_train[:, i]\n",
    "    if i < Nf_lognorm:\n",
    "        histo_range = (0, np.percentile(x_aux, 99.))\n",
    "        bin_content, bin_edges = np.histogram(x_aux, bins=40, range=histo_range)\n",
    "        if np.sum(bin_edges>clip_x_to0) > 0:\n",
    "            i_aux = np.argmin(bin_edges<clip_x_to0)\n",
    "            bin_edges = np.concatenate((np.array([0, clip_x_to0]), bin_edges[i_aux:]))\n",
    "        else:\n",
    "            bin_edges = np.array([0, clip_x_to0])\n",
    "        bin_content, bin_edges = np.histogram(x_aux, bins=bin_edges, range=histo_range)\n",
    "        bincenters = 0.5*(bin_edges[1:]+bin_edges[:-1])\n",
    "        bin_width = bin_edges[1:] - bin_edges[:-1]\n",
    "        bin_width[0] = 1.\n",
    "\n",
    "        mu_vec = pars_ae_train[0][:, i]\n",
    "        sigma_vec = pars_ae_train[1][:, i]\n",
    "        f_vec = pars_ae_train[2][:, i]\n",
    "        spdf_bin_content = np.sum(bin_content)*bin_width*sum_of_lognorm(bincenters, f_vec, mu_vec, sigma_vec)\n",
    "    elif i < Nf_lognorm + Nf_gauss:\n",
    "        histo_range = (np.percentile(x_aux, .1), np.percentile(x_aux, 99.9))\n",
    "        bin_content, bin_edges = np.histogram(x_aux, bins=40, range=histo_range)\n",
    "        bincenters = 0.5*(bin_edges[1:]+bin_edges[:-1])\n",
    "        bin_width = bincenters[1]-bincenters[0]\n",
    "\n",
    "        mu_vec = pars_ae_train[0][:, i]\n",
    "        sigma_vec = pars_ae_train[1][:, i]\n",
    "        spdf_bin_content = np.sum(bin_content)*bin_width*sum_of_gaussians(bincenters, mu_vec, sigma_vec)\n",
    "    elif i < Nf_lognorm + Nf_gauss + Nf_Pgauss:\n",
    "        histo_range = (np.percentile(x_aux, .1), np.percentile(x_aux, 99.9))\n",
    "        bin_content, bin_edges = np.histogram(x_aux, bins=40, range=histo_range)\n",
    "        bincenters = 0.5*(bin_edges[1:]+bin_edges[:-1])\n",
    "        bin_width = bincenters[1]-bincenters[0]\n",
    "\n",
    "        mu_vec = pars_ae_train[0][:, i]\n",
    "        sigma_vec = pars_ae_train[1][:, i]\n",
    "        spdf_bin_content = np.sum(bin_content)*bin_width*sum_of_Pgauss(bincenters, mu_vec, sigma_vec)\n",
    "    elif i < Nf_lognorm + Nf_gauss + Nf_Pgauss + Nf_PDgauss:\n",
    "        Nmax = int(np.percentile(x_aux, 99.9))\n",
    "        bin_content, bin_edges = np.histogram(x_aux, bins=Nmax+1, range=(-0.5, Nmax+0.5))\n",
    "        bincenters = 0.5*(bin_edges[1:]+bin_edges[:-1])\n",
    "        bin_width = bincenters[1]-bincenters[0]\n",
    "\n",
    "        mu_vec = pars_ae_train[0][:, i]\n",
    "        sigma_vec = pars_ae_train[1][:, i]\n",
    "        spdf_bin_content = np.sum(bin_content)*bin_width*sum_of_PDgauss(bincenters, mu_vec, sigma_vec)\n",
    "    elif i < Nf_lognorm + Nf_gauss + Nf_Pgauss + Nf_PDgauss + Nf_binomial:\n",
    "        bin_content, bin_edges = np.histogram(x_aux, bins=2, range=(np.min(x_aux), np.max(x_aux)))\n",
    "        bincenters = 0.5*(bin_edges[1:]+bin_edges[:-1])\n",
    "        bin_width = bincenters[1]-bincenters[0]\n",
    "        p = 0.5*(1+0.98*np.tanh(pars_ae_train[0][:, i]))\n",
    "\n",
    "        spdf_bin_content = np.zeros_like(bincenters)\n",
    "        spdf_bin_content[np.nonzero(bin_content)[0][1]] = np.sum(p)\n",
    "        spdf_bin_content[np.nonzero(bin_content)[0][0]] = np.sum(1-p)\n",
    "    else:\n",
    "        Nmax = int(np.percentile(x_aux, 99.9))\n",
    "        bin_content, bin_edges = np.histogram(x_aux, bins=Nmax+1, range=(-0.5, Nmax+0.5))\n",
    "        bincenters = 0.5*(bin_edges[1:]+bin_edges[:-1])\n",
    "        bin_width = bincenters[1]-bincenters[0]\n",
    "        aux = pars_ae_train[0][:, i]\n",
    "        mu_vec = 1 + np.where(np.greater(aux, 0), aux, np.divide(aux, np.sqrt(1+np.square(aux))))\n",
    "        spdf_bin_content = sum_of_possion(bincenters, mu_vec)\n",
    "\n",
    "\n",
    "    if i < Nf_lognorm:\n",
    "        bin_width[0] = clip_x_to0\n",
    "        \n",
    "    ax_arr[i/3, i%3].errorbar(bincenters, bin_content, xerr=bin_width/2., yerr=np.sqrt(bin_content), fmt='.b', label='input')\n",
    "    ax_arr[i/3, i%3].errorbar(bincenters, spdf_bin_content, xerr=bin_width/2., fmt='.r', label='spdf')\n",
    "    ax_arr[i/3, i%3].grid()\n",
    "    ax_arr[i/3, i%3].set_title(hlf_name + ' - ' + SampleName)        \n",
    "    ax_arr[i/3, i%3].legend(loc='best')\n",
    "    \n",
    "    if hlf_name.startswith('LepIso') or hlf_name.startswith('all'):\n",
    "        ax_arr[i/3, i%3].set_yscale('log')\n",
    "        \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Statistics with Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import loggamma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numpy_KL_loss(mu, sigma):\n",
    "    sp = np.atleast_2d(sigma_prior)\n",
    "    mp = np.atleast_2d(mu_prior)\n",
    "    kl_loss = np.square(sigma)*np.square(sp)\n",
    "    kl_loss += np.square((mp - mu)/sp)\n",
    "    kl_loss += np.log(sp/sigma) -1\n",
    "\n",
    "    return 0.5 * np.sum(kl_loss, axis=-1)\n",
    "\n",
    "def numpy_RecoProb(x, par1, par2, par3):\n",
    "    N = 0\n",
    "    nll_loss = 0\n",
    "    \n",
    "    #Log-Normal distributed variables\n",
    "    mu = par1[:,:Nf_lognorm]\n",
    "    sigma = par2[:,:Nf_lognorm]\n",
    "    fraction = par3[:,:Nf_lognorm]\n",
    "    x_clipped = np.clip(x[:,:Nf_lognorm], clip_x_to0, 1e8)\n",
    "    single_NLL = np.where(np.less(x[:,:Nf_lognorm], clip_x_to0), \n",
    "                            -np.log(fraction),\n",
    "                                -np.log(1-fraction)\n",
    "                                + np.log(sigma) \n",
    "                                + np.log(x_clipped)\n",
    "                                + 0.5*np.square((np.log(x_clipped) - mu) / sigma)\n",
    "                           )\n",
    "    nll_loss += np.sum(single_NLL, axis=-1)\n",
    "    N += Nf_lognorm\n",
    "    \n",
    "    # Gaussian distributed variables\n",
    "    mu = par1[:,N:N+Nf_gauss]\n",
    "    sigma = par2[:,N:N+Nf_gauss]\n",
    "    norm_x = (x[:,N:N+Nf_gauss] - mu)/sigma\n",
    "    single_NLL = np.log(sigma) + 0.5*np.square(norm_x)\n",
    "    nll_loss += np.sum(single_NLL, axis=-1)\n",
    "    N += Nf_gauss\n",
    "    \n",
    "    # Positive Gaussian distributed variables\n",
    "    mu = par1[:,N:N+Nf_Pgauss]\n",
    "    sigma = par2[:,N:N+Nf_Pgauss]\n",
    "    norm_x = (x[:,N:N+Nf_Pgauss] - mu)/sigma\n",
    "\n",
    "    sqrt2 = 1.4142135624\n",
    "    aNorm = 1 + 0.5*(1 + erf( -mu/(sigma*sqrt2) ))\n",
    "    \n",
    "    single_NLL = np.log(sigma) + 0.5*np.square(norm_x) - np.log(aNorm)\n",
    "    nll_loss += np.sum(single_NLL, axis=-1)\n",
    "    N += Nf_Pgauss\n",
    "    \n",
    "    # Positive Discrete Gaussian distributed variables\n",
    "    mu = par1[:,N:N+Nf_PDgauss]\n",
    "    sigma = par2[:,N:N+Nf_PDgauss]\n",
    "    norm_xp = (x[:,N:N+Nf_PDgauss] + 0.5 - mu)/sigma\n",
    "    norm_xm = (x[:,N:N+Nf_PDgauss] - 0.5 - mu)/sigma\n",
    "    sqrt2 = 1.4142135624\n",
    "    single_LL = 0.5*(erf(norm_xp/sqrt2) - erf(norm_xm/sqrt2))\n",
    "    \n",
    "    norm_0 = (-0.5 - mu)/sigma\n",
    "    aNorm = 1 + 0.5*(1 + erf(norm_0/sqrt2))\n",
    "    \n",
    "    single_NLL = -np.log(np.clip(single_LL, 1e-10, 1e40)) -np.log(aNorm)\n",
    "    nll_loss += np.sum(single_NLL, axis=-1)\n",
    "    N += Nf_PDgauss\n",
    "    \n",
    "    #Binomial distributed variables\n",
    "    p = 0.5*(1+0.98*np.tanh(par1[:, N: N+Nf_binomial]))\n",
    "    single_NLL = -np.where(np.equal(x[:, N: N+Nf_binomial],1), np.log(p), np.log(1-p))\n",
    "    nll_loss += np.sum(single_NLL, axis=-1)\n",
    "    N += Nf_binomial\n",
    "    \n",
    "    #Poisson distributed variables\n",
    "    aux = par1[:, N:]\n",
    "    mu = 1 + np.where(np.greater(aux, 0), aux, aux/np.sqrt(1+np.square(aux)))\n",
    "    single_NLL = loggamma(x[:, N:]+1).real - x[:, N:]*np.log(mu) + mu\n",
    "    nll_loss += np.sum(single_NLL, axis=-1)\n",
    "    \n",
    "    return nll_loss\n",
    "\n",
    "def numpy_loss(x, x_encoded, pars_ae):\n",
    "    mu = x_encoded[0]\n",
    "    sigma = x_encoded[1]    \n",
    "    kl_loss = numpy_KL_loss(mu, sigma)\n",
    "    \n",
    "    nll_loss = numpy_RecoProb(x, pars_ae[0], pars_ae[1], pars_ae[2])\n",
    "    \n",
    "    return [nll_loss + kl_loss, nll_loss, kl_loss]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train = numpy_loss(x_train, x_train_encoded, pars_ae_train)\n",
    "\n",
    "loss_sig = {}\n",
    "for n in x_sig.keys():\n",
    "    print n\n",
    "    loss_sig[n] = numpy_loss(x_sig[n], x_sig_encoded[n], pars_ae_sig[n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "histos = []\n",
    "canvases = []\n",
    "\n",
    "for loss_piece in range(3):\n",
    "    binning = [100, np.min(loss_train[loss_piece]), np.percentile(loss_train[loss_piece], 100*(1-5e-5))]\n",
    "    print binning\n",
    "    c = rt.TCanvas('c'+str(loss_piece), 'c'+str(loss_piece), 800, 600)\n",
    "\n",
    "\n",
    "    h = rt.TH1F('h_loss_SM{}_{}'.format(SampleName, loss_piece), SampleName, binning[0], binning[1], binning[2])\n",
    "    rtnp.fill_hist(h, loss_train[loss_piece])\n",
    "    h.Scale(1./loss_train[loss_piece].shape[0])\n",
    "\n",
    "    h.SetStats(0)\n",
    "    h.SetLineColor(8)\n",
    "    h.SetFillColorAlpha(8, 0.7)\n",
    "    h.SetFillStyle(3001)\n",
    "    h.Draw('Bar SAME')\n",
    "    histos.append(h)\n",
    "\n",
    "    val_to_cut = np.percentile(loss_train[loss_piece], 100*(1-1e-3))\n",
    "    h.SetTitle(h.GetTitle()+' ({:1.1e})'.format(1e-4))\n",
    "#     print val_to_cut\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    colors = [1,2,4,rt.kYellow+2,6,7, 46]\n",
    "    # fill\n",
    "    for i, n in enumerate(x_sig.keys()):\n",
    "        z = loss_sig[n][loss_piece]\n",
    "        eff = np.sum(z>val_to_cut)/float(z.shape[0])\n",
    "#         print n, ': {:1.2e}'.format(eff)\n",
    "        h1 = rt.TH1F('h_loss_BSM{}_{}'.format(i, loss_piece), n+' ({:1.1e})'.format(eff), binning[0], binning[1], binning[2])\n",
    "        rtnp.fill_hist(h1, z)\n",
    "        h1.Scale(1./float(z.shape[0]))\n",
    "\n",
    "        h1.SetStats(0)\n",
    "        h1.SetLineColor(colors[i])\n",
    "        h1.SetLineWidth(2)\n",
    "        h1.Draw('SAME')\n",
    "        histos.append(h1)\n",
    "\n",
    "    c.BuildLegend()\n",
    "    h.SetTitle('')\n",
    "    Ytitles = ['Loss tot', 'nll(reco probability)', 'KL divergence']\n",
    "    h.SetXTitle(Ytitles[loss_piece])\n",
    "    h.SetYTitle('Probability')\n",
    "\n",
    "    line = rt.TLine()\n",
    "    line.SetLineColor(rt.kYellow+1)\n",
    "    line.SetLineWidth(3)\n",
    "    line.SetLineStyle(9)\n",
    "    line.DrawLine(val_to_cut,0, val_to_cut,1)\n",
    "\n",
    "    c.SetGrid()\n",
    "    c.SetLogy()\n",
    "    c.Draw()\n",
    "    canvases.append(c)\n",
    "    \n",
    "fout = rt.TFile('plots/'+SampleName+ '_v3-1_SWAN_losses.root', 'RECREATE')\n",
    "for obj in canvases + histos:\n",
    "    obj.Write()\n",
    "\n",
    "fout.Close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "fout = h5py.File('losses/'+SampleName+ '_v3-1_array.h5', \"w\")\n",
    "fout.create_dataset(SampleName, data=np.array(loss_train))\n",
    "\n",
    "for k,v in loss_sig.items():\n",
    "    print k\n",
    "    fout.create_dataset(k, data=np.array(v))\n",
    "    \n",
    "fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = h5py.File('losses/ttbar_v3-1_array.h5', 'r')\n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
