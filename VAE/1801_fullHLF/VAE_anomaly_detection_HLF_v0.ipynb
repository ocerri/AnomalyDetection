{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "from glob import glob\n",
    "import sys, scipy\n",
    "from scipy.stats import chi2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, Dense, Lambda, Layer\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras import metrics\n",
    "from keras import optimizers\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf_features = ['Energy', 'Px', 'Py', 'Pz', 'Pt', 'Eta', 'Phi', \n",
    "                'vtxX', 'vtxY', 'vtxZ','ChPFIso', 'GammaPFIso', 'NeuPFIso',\n",
    "                'isChHad', 'isNeuHad', 'isGamma', 'isEle',  'isMu', \n",
    "                #'Charge'\n",
    "               ]\n",
    "hlf_features = ['HT', 'MET', 'phiMET', 'MT', 'nJets', 'nBjets',\n",
    "                'LepPt', 'LepEta', 'LepPhi', 'LepIsoCh',\n",
    "                'LepIsoGamma', 'LepIsoNeu', 'LepCharge', 'LepIsEle'\n",
    "               ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start the VAE declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "original_dim = len(hlf_features)\n",
    "latent_dim = 2\n",
    "intermediate_dim = 40\n",
    "epsilon_std = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Input(shape=(original_dim,))\n",
    "h = Dense(intermediate_dim, activation='tanh')(x)\n",
    "z_mean = Dense(latent_dim)(h)\n",
    "z_log_var = Dense(latent_dim)(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,\n",
    "                              stddev=epsilon_std)\n",
    "    return z_mean + K.exp(z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_h = Dense(intermediate_dim, activation='tanh')\n",
    "decoder_mean = Dense(original_dim)\n",
    "decoder_log_var = Dense(original_dim)\n",
    "\n",
    "h_decoded = decoder_h(z)\n",
    "x_decoded_mean = decoder_mean(h_decoded)\n",
    "x_decoded_log_var = decoder_log_var(h_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom loss layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVariationalLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(CustomVariationalLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def vae_loss(self, x, x_decoded_mean, x_decoded_log_var):\n",
    "        norm_x = K.tf.divide(x-x_decoded_mean, K.exp(x_decoded_log_var))\n",
    "        single_L = 2*x_decoded_log_var + 0.5*K.square(norm_x)\n",
    "        nll_loss = K.sum(single_L, axis=-1)\n",
    "        kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)\n",
    "        return K.mean(nll_loss + kl_loss)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        x_decoded_log_var = inputs[2]\n",
    "        loss = self.vae_loss(x, x_decoded_mean, x_decoded_log_var)\n",
    "        self.add_loss(loss, inputs=inputs)\n",
    "        # We won't actually use the output.\n",
    "        return x\n",
    "    \n",
    "y = CustomVariationalLayer()([x, x_decoded_mean, x_decoded_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = Model(x, y)\n",
    "vae.compile(optimizer='adam', loss=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = '~/cernbox/AnomalyDetection/data/MaxLepDeltaR_des/'\n",
    "\n",
    "train_list = glob(data_folder +'train/*.h5')\n",
    "hlf_train = np.zeros((0,14))\n",
    "label_train = np.zeros((0,))\n",
    "for fname in train_list:\n",
    "    f = h5py.File(fname)\n",
    "    hlf_train = np.concatenate((hlf_train, np.array(f['HLF'])))\n",
    "    label_train = np.concatenate((label_train, np.array(f['Labels'])[:,1] + 2*np.array(f['Labels'])[:,2]))\n",
    "\n",
    "cut = label_train==0\n",
    "x_train = hlf_train[cut]\n",
    "l_train = label_train[cut]\n",
    "print 'Train data shape: ', x_train.shape\n",
    "\n",
    "\n",
    "\n",
    "val_list = glob(data_folder +'val/*.h5')\n",
    "hlf_val = np.zeros((0,14))\n",
    "label_val = np.zeros((0,))\n",
    "for fname in val_list:\n",
    "    f = h5py.File(fname)\n",
    "    hlf_val = np.concatenate((hlf_val, np.array(f['HLF'])))\n",
    "    label_val = np.concatenate((label_val, np.array(f['Labels'])[:,1] + 2*np.array(f['Labels'])[:,2]))\n",
    "\n",
    "x_test = hlf_val #[f_val['Labels'][:,0]==1,:]\n",
    "print 'Val data shape: ', x_test.shape\n",
    "\n",
    "# print f_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_report = vae.fit(x_train,\n",
    "        shuffle=True,\n",
    "        epochs=200,\n",
    "        batch_size=batch_size,\n",
    "        validation_data=(x_test, None),\n",
    "        callbacks = [\n",
    "            EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
    "            ReduceLROnPlateau(monitor='loss', factor=0.3, patience=5, verbose=1)\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ['loss', 'val_loss']:\n",
    "    plt.semilogy(fit_report.history[item][10:], label=item)\n",
    "plt.xlabel('Epoch')\n",
    "plt.grid()\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build encoder and decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(inputs=x, outputs=[z_mean, z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_encoded = np.array(encoder.predict(x_train, batch_size=batch_size))\n",
    "x_train_encoded.shape\n",
    "plt.figure()\n",
    "plt.scatter(x_train_encoded[0, :, 0], x_train_encoded[0, :, 1], c=l_train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test_encoded = np.array(encoder.predict(x_test, batch_size=batch_size))\n",
    "x_test_encoded.shape\n",
    "plt.figure()\n",
    "plt.scatter(x_test_encoded[0, :, 0], x_test_encoded[0, :, 1], c=label_val)\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probabilistic Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqrt_2pi = np.sqrt(2*np.pi)\n",
    "class GaussProbabilityComputerLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.is_placeholder = True\n",
    "        super(GaussProbabilityComputerLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = inputs[0]\n",
    "        x_decoded_mean = inputs[1]\n",
    "        x_decoded_log_var = inputs[2]\n",
    "        norm_x = K.tf.divide(x-x_decoded_mean, K.exp(x_decoded_log_var))\n",
    "        exp_part = K.exp(-0.5*K.square(norm_x))\n",
    "        prob = K.tf.divide(exp_part,K.exp(x_decoded_log_var))/sqrt_2pi\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input = Input(shape=(latent_dim,))\n",
    "_h_decoded = decoder_h(decoder_input)\n",
    "_x_decoded_mean = decoder_mean(_h_decoded)\n",
    "_x_decoded_log_var = decoder_log_var(_h_decoded)\n",
    "decoder = Model(decoder_input, [_x_decoded_mean, _x_decoded_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gaussian_np(x, mu=0, sigma=1):\n",
    "    z = (x-mu)/sigma\n",
    "    return np.exp(-0.5*np.square(z))/(sigma*np.sqrt(2*np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(np.abs(Gaussian_np(np.random.uniform(size=(400)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_aux = x_train[:1,:]\n",
    "print 'aux:', aux\n",
    "pred = encoder.predict(aux)\n",
    "print 'Latent Space'\n",
    "print pred[0]\n",
    "print np.exp(pred[1])\n",
    "\n",
    "z_aux = np.random.normal(loc=pred[0], scale=np.exp(pred[1]))\n",
    "print z_aux\n",
    "\n",
    "print 'Predicted x'\n",
    "pred_x = decoder.predict(z)\n",
    "mu = pred_x[0]\n",
    "sigma = np.exp(pred_x[1])\n",
    "print mu\n",
    "print sigma\n",
    "x_norm = (x_aux-mu)/sigma\n",
    "print x_norm\n",
    "pvalue = 1-np.abs(scipy.special.erf(x_norm))/np.sqrt(2)\n",
    "print 'pvalues', pvalue\n",
    "\n",
    "comb_pvalue = 1-chi2.cdf(-np.sum(np.log(pvalue)), 2*pvalue.shape[1])\n",
    "print 'Event combined p-value:', comb_pvalue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_pvalue = []\n",
    "for x_aux in x_train:\n",
    "    pred = encoder.predict(aux)\n",
    "    z_aux = np.random.normal(loc=pred[0], scale=np.exp(pred[1]))\n",
    "    pred_x = decoder.predict(z)\n",
    "    mu = pred_x[0]\n",
    "    sigma = np.exp(pred_x[1])\n",
    "    x_norm = (x_aux-mu)/sigma\n",
    "    pvalue = 1-np.abs(scipy.special.erf(x_norm))/np.sqrt(2)\n",
    "    comb_pvalue.append(1-chi2.cdf(-np.sum(np.log(pvalue)), 2*pvalue.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(comb_pvalue, bins=20)\n",
    "plt.yscale('log', nonposy='clip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_gaus_prob = GaussProbabilityComputerLayer()([x, _x_decoded_mean, _x_decoded_log_var])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
